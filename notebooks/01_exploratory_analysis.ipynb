{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Test - Data Scientist\n",
    "## Sales Analysis and Prediction\n",
    "\n",
    "### Context\n",
    "Sales analysis of a retail chain to build a predictive model that estimates the number of units that will be sold in the future.\n",
    "\n",
    "### Dataset: sales_data.xlsx\n",
    "- **Date**: Date of sale\n",
    "- **Store**: Unique store identifier\n",
    "- **Category**: Product category\n",
    "- **Units_Sold**: Number of units sold\n",
    "- **Unit_Price**: Price per unit\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Library Import\n",
    "\n",
    "First, we will import all necessary libraries for exploratory analysis, preprocessing, visualization, and modeling.\n",
    "\n",
    "---\n",
    "### ‚ö†Ô∏è IMPORTANT - BEFORE STARTING:\n",
    "\n",
    "If you are running this notebook for the first time or after making changes:\n",
    "\n",
    "1. **Restart the kernel**: `Kernel ‚Üí Restart Kernel` or `Ctrl+Shift+P ‚Üí Restart Kernel`\n",
    "2. **Run all cells in order**: `Cell ‚Üí Run All` or run cell by cell from the beginning\n",
    "\n",
    "This ensures all variables are loaded correctly.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Libraries for modeling\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Libraries for evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Data Loading\n",
    "\n",
    "We will load the sales dataset to begin our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: Adjust path if necessary\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path to import modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from data_preprocessing import load_data\n",
    "\n",
    "df = load_data('../data/raw/sales_data.xlsx')\n",
    "\n",
    "# Show basic information\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASIC DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset dimensions: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "print(f\"\\nAvailable columns:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIRST 10 ROWS OF THE DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "## 3.1 General Dataset Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed dataset information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\" * 80)\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Descriptive Statistics\n",
    "\n",
    "Statistical analysis of the numerical variables in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete descriptive statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS - Numerical Variables\")\n",
    "print(\"=\" * 80)\n",
    "statistics = df.describe()\n",
    "print(statistics)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS - Categorical Variables\")\n",
    "print(\"=\" * 80)\n",
    "cat_statistics = df.describe(include=['object'])\n",
    "print(cat_statistics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Missing Values and Duplicates\n",
    "\n",
    "Data quality verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUES PER COLUMN\")\n",
    "print(\"=\" * 80)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "missing_table = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage (%)': missing_percentage\n",
    "})\n",
    "print(missing_table)\n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DUPLICATE VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "print(f\"Percentage of duplicates: {(duplicates / len(df)) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Correlation Analysis\n",
    "\n",
    "We will analyze the relationships between numerical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numerical columns found: {numeric_columns}\")\n",
    "\n",
    "if len(numeric_columns) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No numerical columns found.\")\n",
    "else:\n",
    "    # Calculate correlation matrix\n",
    "    correlation = df[numeric_columns].corr()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORRELATION MATRIX\")\n",
    "    print(\"=\" * 80)\n",
    "    print(correlation)\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    if len(numeric_columns) > 1:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n",
    "                    square=True, linewidths=1, fmt='.3f')\n",
    "        plt.title('Correlation Matrix - Numerical Variables', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Only one numerical variable, cannot calculate correlation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Distribution of Numerical Variables\n",
    "\n",
    "We will analyze the distribution of each numerical variable to identify patterns and outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df[col], kde=True, bins=20)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    # Boxplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Categorical Variable Analysis\n",
    "\n",
    "We will analyze the frequency of categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col != 'Date':  # Skip Date for now as it has many unique values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(y=df[col], order=df[col].value_counts().index)\n",
    "        plt.title(f'Frequency of {col}')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Data Preprocessing\n",
    "\n",
    "We will use our custom module to clean and prepare the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessing import run_preprocessing_pipeline\n",
    "\n",
    "# Run the full pipeline\n",
    "data = run_preprocessing_pipeline('../data/raw/sales_data.xlsx')\n",
    "\n",
    "# Extract processed data\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä Important Context: Small Dataset\n",
    "- Explains why simple models are better with 55 records\n",
    "- Sets correct expectations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Modeling\n",
    "\n",
    "We will train and evaluate multiple models to find the best one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling import run_modeling_pipeline\n",
    "\n",
    "# Run the modeling pipeline\n",
    "modeling_results = run_modeling_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç ANALYSIS: WHY IS THIS MODEL THE BEST WITH 55 RECORDS?\")\n",
    "print(\"- Automatic Bias-Variance Trade-off analysis\")\n",
    "print(\"- Technical comparison Linear Regression vs Random Forest\")\n",
    "print(\"- Explanation of underfitting in complex models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÜ Why the Simple Model Won: Deep Analysis\n",
    "- Complete and defensible argumentation\n",
    "- Prepared answers for interview questions\n",
    "- Validation of ML principles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Conclusions\n",
    "\n",
    "The analysis is complete. Please refer to the executive summary in the modeling output for final recommendations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
